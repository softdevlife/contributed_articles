![hierarchicaltablessavings](https://user-images.githubusercontent.com/12673581/60295407-37910980-9956-11e9-8342-9cccbea1dbe0.png)

![incrementalsavings](https://user-images.githubusercontent.com/12673581/60295439-45468f00-9956-11e9-8439-db278ed6176f.png)

Google BigQuery sounds to be too expensive at first. Given that the files stored in BigQuery cannot be compressed and the current cost is $5 per 1TB queried and $20 per month for every 1TB stored, the bill becomes huge when multiple users often query denormalized tables in there. In contrast, there are other alternative services that are similar to BigQuery like AWS Athena which allows you to query tables in compressed format, as well the ability to query those tables that are stored in the data lake through other compute options, such AWS Glue, AWS EMR, or by provisioning your own Hadoop cluster. These other flexible compute choices eliminates the cost on relying to query the tables on AWS Athena for $5 per 1TB queried and instead gives us the choice to use other cheaper compute options.

As depicted to the pictures above though, the costs can become affordable by reducing the billing costs from several hundred of dollars to a few dollars per day if users avoid querying their stuff in a **denormalized table** and not **aggregating the same results several times**. The main reason others have told you that BigQuery is expensive is that they mostly free roam around querying around the tables that they have imported without having some policy or guidelines in place. Enforcing best practices within the BigQuery projects that you maintain is critical for avoiding the same experience others have foretold you for BigQuery being too expensive. Fortunately, BigQuery over the years has added several features in enabling users in reducing their query costs without requiring to use any programming language at all.

Some of the ways of reducing cost in BigQuery are:

* **Aggregating data only once:** Instead of aggregating the raw data several times, it is best to store the calculations of the aggregations in a staging table and update it every day incrementally. For example, instead of aggregating for the last 90 days every day, aggregate only for today’s date and store it in a staging table where you will instead query from. That way, you retrieve fewer data from the raw table every day, reducing the query costs. BigQuery allows regular users to automate the task of incrementally updating their staging table by [scheduling queries](https://cloud.google.com/bigquery/docs/scheduling-queries)
* **Converting defined dimensions into metrics:** If we have within our staging table a lot of dimensions that are in high cardinality, we will still have a lot of records to compute. To reduce the number of records, check if it is possible for some dimensions to be converted into metrics that we only need.
* **Generalize existing dimensions:** Otherwise, if the dimension is still important to remain, yet it has high cardinality, try to reduce it by generalizing the several values within said dimension into few categories. That way, the number of records will get decreased.
* **Maintain staging tables with DML statements:** There will be a lot of occasions where you will need to add new columns within your existing staging table or update existing columns due to business logic changes. Instead of creating a new table again from scratch with the raw data, it is more cost effective on maintaining it when possible. [BigQuery has DML statements](https://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax) like MERGE/DELETE/INSERT/UPDATE to handle that.
* **Using Partition columns:** When we filter our data in BigQuery, the cost of the query will be as if we were scanning the whole table. With setting [partitions](https://cloud.google.com/bigquery/docs/partitioned-tables) on our table, every time we filter with that partition column, we scan only the records the filter picks up, saving us a lot of costs. Make sure the staging table or any other table that gets created has a date partition column that you will usually often want to use as a filter.
* **Storing raw data in a hierarchical format with nested fields:** Instead of having several tables all denormalized into one, it is better whenever possible to store each table as a nested field within a table that stores such data in a hierarchical format. A table with nested fields gets the same benefit as a denormalized table that does not require the need to do joins leaving BigQuery to stay out of trouble on using computing resources to make the linking between tables on the fly. Unlike denormalized tables, tables with nested fields reduce data redundancy by keeping the data normalized. For a better explanation, I recommend taking a read of the article [Why Nesting Is So Cool](https://looker.com/blog/why-nesting-is-so-cool) from data-discovery platform Looker.
* **Cluster Columns:** [Cluster columns](https://cloud.google.com/bigquery/docs/clustered-tables) are not as effective as partition columns on saving query costs, but are still a great choice if you still have a lot of data to scan within each partition and the column that you cluster has high cardinality.
* **Monitor Logs:** Have detail and summary reports so users can have visibility of their daily usage. [stackdriver logging](https://cloud.google.com/logging/docs/) allows you to import Bigquery activity logs within BigQuery and [Google provides several good views examples that you can create from the logs provided](https://cloud.google.com/bigquery/docs/reference/auditlogs/#querying-exported-logs).
* **Most queries should be considered experimental:** If any query becomes almost like a production run, it is always good to check if the same metrics/dimensions are not created multiple times from other queries as well. Otherwise, it should instead be done only once by a team that maintains them. Make sure the scheduled queries are stopped when the deliverables produced are not in use and the tables created have an [expiry date by updating the table’s description](https://cloud.google.com/bigquery/docs/managing-tables#updating_a_tables_description).
* **Only get the records and columns that are in need:** Make sure users do proper research of the data sources that they are working with so they can retrieve only the data that they will need.
* **Train users to learn BigQuery:** Users will get the most on optimizing their queries by having actually tried most of what they can do with BigQuery. [Qwiklabs has a lot of BigQuery hands-on labs](https://www.qwiklabs.com/catalog?keywords=bigquery) that you can complete each at your own pace where each lab takes less than an hour or two. The quests that we recommend to start with are [BigQuery Basics for Data Analysis](https://google.qwiklabs.com/quests/69), [BigQuery for Marketing Analysts](https://google.qwiklabs.com/quests/70), [BigQuery for Data Warehousing](https://google.qwiklabs.com/quests/68), and [BigQuery for Machine Learning](https://google.qwiklabs.com/quests/71).

**Conclusion**

BigQuery will still not be as cheap to other service providers after following best practices, but the trade-offs to what you will be receiving are second to none. They offer the best user experience and ease of use where even a regular user can pull out stuff that you used to require the assistance of a Data Engineer/Machine Learning Engineer. In terms of performance, I have never found so far a data-warehouse as BigQuery where you can stack so many stuff inside a query while still getting the results in less than five minutes. By enforcing best practices that we have discussed so far within the BigQuery projects that you have to maintain, the costs will start to become more affordable.

Did you have fun reading my answer to whether BigQuery is expensive? If so, you can read [my full-length article on Medium on optimizing queries within BigQuery](https://medium.com/@SoftDevLife/optimising-queries-in-bigquery-for-beginners-971be491f1de). In my article, you will learn what BigQuery contains under the hood and get more details to the topics that we have discussed so far. You will also get a step by step guide on how to run efficient queries by aggregating the data only once from a public session dataset that Google provides.
